diff --git a/camus-api/src/main/java/com/linkedin/camus/coders/CamusWrapper.java b/camus-api/src/main/java/com/linkedin/camus/coders/CamusWrapper.java
index e75a576..9ed8ec2 100644
--- a/camus-api/src/main/java/com/linkedin/camus/coders/CamusWrapper.java
+++ b/camus-api/src/main/java/com/linkedin/camus/coders/CamusWrapper.java
@@ -1,7 +1,6 @@
 package com.linkedin.camus.coders;
 
 import org.apache.hadoop.io.MapWritable;
-import org.apache.hadoop.io.Text;
 import org.apache.hadoop.io.Writable;
 
 /**
@@ -10,20 +9,20 @@ import org.apache.hadoop.io.Writable;
  *
  * @author kgoodhop
  *
- * @param <R> The type of decoded payload
+ * @param <MESSAGE> The type of decoded payload
  */
-public class CamusWrapper<R> extends CamusWrapperLight<R> {
+public class CamusWrapper<MESSAGE> extends CamusWrapperBase<MESSAGE> {
     private MapWritable partitionMap;
 
-    public CamusWrapper(R record) {
+    public CamusWrapper(MESSAGE record) {
         this(record, System.currentTimeMillis());
     }
 
-    public CamusWrapper(R record, long timestamp) {
+    public CamusWrapper(MESSAGE record, long timestamp) {
         this(record, timestamp, "unknown_server", "unknown_service");
     }
 
-    public CamusWrapper(R record, long timestamp, String server, String service) {
+    public CamusWrapper(MESSAGE record, long timestamp, String server, String service) {
         super(record, timestamp);
 //        this.partitionMap = new MapWritable();
 //        partitionMap.put(new Text("server"), new Text(server));
diff --git a/camus-api/src/main/java/com/linkedin/camus/coders/CamusWrapperBase.java b/camus-api/src/main/java/com/linkedin/camus/coders/CamusWrapperBase.java
new file mode 100644
index 0000000..be6ed7f
--- /dev/null
+++ b/camus-api/src/main/java/com/linkedin/camus/coders/CamusWrapperBase.java
@@ -0,0 +1,27 @@
+package com.linkedin.camus.coders;
+
+public class CamusWrapperBase<MESSAGE> {
+    protected MESSAGE record;
+    protected long timestamp;
+
+    public CamusWrapperBase(MESSAGE record, long timestamp) {
+        this.record = record;
+        this.timestamp = timestamp;
+    }
+
+    /**
+     * Returns the payload record for a single message
+     * @return
+     */
+    public MESSAGE getRecord() {
+        return record;
+    }
+
+    /**
+     * Returns current if not set by the decoder
+     * @return
+     */
+    public long getTimestamp() {
+        return timestamp;
+    }
+}
diff --git a/camus-api/src/main/java/com/linkedin/camus/coders/CamusWrapperLight.java b/camus-api/src/main/java/com/linkedin/camus/coders/CamusWrapperLight.java
deleted file mode 100644
index 0a5030d..0000000
--- a/camus-api/src/main/java/com/linkedin/camus/coders/CamusWrapperLight.java
+++ /dev/null
@@ -1,27 +0,0 @@
-package com.linkedin.camus.coders;
-
-public class CamusWrapperLight<R> {
-    protected R record;
-    protected long timestamp;
-
-    public CamusWrapperLight(R record, long timestamp) {
-        this.record = record;
-        this.timestamp = timestamp;
-    }
-
-    /**
-     * Returns the payload record for a single message
-     * @return
-     */
-    public R getRecord() {
-        return record;
-    }
-
-    /**
-     * Returns current if not set by the decoder
-     * @return
-     */
-    public long getTimestamp() {
-        return timestamp;
-    }
-}
diff --git a/camus-api/src/main/java/com/linkedin/camus/coders/KeyedCamusWrapper.java b/camus-api/src/main/java/com/linkedin/camus/coders/KeyedCamusWrapper.java
new file mode 100644
index 0000000..5287858
--- /dev/null
+++ b/camus-api/src/main/java/com/linkedin/camus/coders/KeyedCamusWrapper.java
@@ -0,0 +1,15 @@
+package com.linkedin.camus.coders;
+
+public class KeyedCamusWrapper<KEY, MESSAGE> extends CamusWrapperBase<MESSAGE> {
+
+    protected KEY key;
+
+    public KeyedCamusWrapper(KEY key, MESSAGE record, long timestamp) {
+        super(record, timestamp);
+        this.key = key;
+    }
+
+    public KEY getKey() {
+        return key;
+    }
+}
diff --git a/camus-api/src/main/java/com/linkedin/camus/coders/KeyedMessageDecoder.java b/camus-api/src/main/java/com/linkedin/camus/coders/KeyedMessageDecoder.java
new file mode 100644
index 0000000..6f4b4d2
--- /dev/null
+++ b/camus-api/src/main/java/com/linkedin/camus/coders/KeyedMessageDecoder.java
@@ -0,0 +1,5 @@
+package com.linkedin.camus.coders;
+
+public abstract class KeyedMessageDecoder<K_IN, K_OUT, M_IN, M_OUT> extends MessageDecoder<M_OUT, M_IN> {
+    public abstract KeyedCamusWrapper<K_OUT, M_OUT> decode(K_IN key, M_IN message);
+}
diff --git a/camus-api/src/main/java/com/linkedin/camus/etl/RecordWriterProvider.java b/camus-api/src/main/java/com/linkedin/camus/etl/RecordWriterProvider.java
index e7d4abc..68ee392 100644
--- a/camus-api/src/main/java/com/linkedin/camus/etl/RecordWriterProvider.java
+++ b/camus-api/src/main/java/com/linkedin/camus/etl/RecordWriterProvider.java
@@ -2,7 +2,8 @@ package com.linkedin.camus.etl;
 
 import java.io.IOException;
 
-import com.linkedin.camus.coders.CamusWrapperLight;
+import com.linkedin.camus.coders.CamusWrapperBase;
+import com.linkedin.camus.coders.KeyedCamusWrapper;
 import org.apache.hadoop.mapreduce.RecordWriter;
 import org.apache.hadoop.mapreduce.TaskAttemptContext;
 import org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter;
@@ -15,7 +16,7 @@ public interface RecordWriterProvider {
 
     String getFilenameExtension();
 
-    RecordWriter<IEtlKey, CamusWrapperLight> getDataRecordWriter(
-            TaskAttemptContext context, String fileName, CamusWrapperLight data, FileOutputCommitter committer) throws IOException,
+    RecordWriter<IEtlKey, KeyedCamusWrapper> getDataRecordWriter(
+            TaskAttemptContext context, String fileName, CamusWrapperBase data, FileOutputCommitter committer) throws IOException,
             InterruptedException;
 }
diff --git a/camus-etl-kafka/src/main/java/com/linkedin/camus/etl/kafka/coders/MessageDecoderFactory.java b/camus-etl-kafka/src/main/java/com/linkedin/camus/etl/kafka/coders/MessageDecoderFactory.java
index 3803af3..3b4dd3e 100644
--- a/camus-etl-kafka/src/main/java/com/linkedin/camus/etl/kafka/coders/MessageDecoderFactory.java
+++ b/camus-etl-kafka/src/main/java/com/linkedin/camus/etl/kafka/coders/MessageDecoderFactory.java
@@ -11,10 +11,10 @@ import com.linkedin.camus.etl.kafka.mapred.EtlInputFormat;
 
 public class MessageDecoderFactory {
     
-    public static MessageDecoder<?,?> createMessageDecoder(JobContext context, String topicName){
-        MessageDecoder<?,?> decoder;
+    public static MessageDecoder createMessageDecoder(JobContext context, String topicName){
+        MessageDecoder decoder;
         try {
-            decoder = (MessageDecoder<?,?>) EtlInputFormat.getMessageDecoderClass(context).newInstance();
+            decoder = EtlInputFormat.getMessageDecoderClass(context).newInstance();
             
             Properties props = new Properties();
             for (Entry<String, String> entry : context.getConfiguration()){
diff --git a/camus-etl-kafka/src/main/java/com/linkedin/camus/etl/kafka/common/AvroRecordWriterProvider.java b/camus-etl-kafka/src/main/java/com/linkedin/camus/etl/kafka/common/AvroRecordWriterProvider.java
index aa970f6..0b14c8d 100644
--- a/camus-etl-kafka/src/main/java/com/linkedin/camus/etl/kafka/common/AvroRecordWriterProvider.java
+++ b/camus-etl-kafka/src/main/java/com/linkedin/camus/etl/kafka/common/AvroRecordWriterProvider.java
@@ -1,6 +1,7 @@
 package com.linkedin.camus.etl.kafka.common;
 
-import com.linkedin.camus.coders.CamusWrapperLight;
+import com.linkedin.camus.coders.CamusWrapperBase;
+import com.linkedin.camus.coders.KeyedCamusWrapper;
 import com.linkedin.camus.etl.IEtlKey;
 import com.linkedin.camus.etl.RecordWriterProvider;
 import com.linkedin.camus.etl.kafka.mapred.EtlMultiOutputFormat;
@@ -31,10 +32,10 @@ public class AvroRecordWriterProvider implements RecordWriterProvider {
     }
 
     @Override
-    public RecordWriter<IEtlKey, CamusWrapperLight> getDataRecordWriter(
+    public RecordWriter<IEtlKey, KeyedCamusWrapper> getDataRecordWriter(
             TaskAttemptContext context,
             String fileName,
-            CamusWrapperLight data,
+            CamusWrapperBase data,
             FileOutputCommitter committer) throws IOException, InterruptedException {
         final DataFileWriter<Object> writer = new DataFileWriter<Object>(
                 new SpecificDatumWriter<Object>());
@@ -55,9 +56,9 @@ public class AvroRecordWriterProvider implements RecordWriterProvider {
 
         writer.setSyncInterval(EtlMultiOutputFormat.getEtlAvroWriterSyncInterval(context));
 
-        return new RecordWriter<IEtlKey, CamusWrapperLight>() {
+        return new RecordWriter<IEtlKey, KeyedCamusWrapper>() {
             @Override
-            public void write(IEtlKey ignore, CamusWrapperLight data) throws IOException {
+            public void write(IEtlKey ignore, KeyedCamusWrapper data) throws IOException {
                 writer.append(data.getRecord());
             }
 
diff --git a/camus-etl-kafka/src/main/java/com/linkedin/camus/etl/kafka/common/SequenceFileRecordWriterProvider.java b/camus-etl-kafka/src/main/java/com/linkedin/camus/etl/kafka/common/SequenceFileRecordWriterProvider.java
index d7ea53f..31311e3 100644
--- a/camus-etl-kafka/src/main/java/com/linkedin/camus/etl/kafka/common/SequenceFileRecordWriterProvider.java
+++ b/camus-etl-kafka/src/main/java/com/linkedin/camus/etl/kafka/common/SequenceFileRecordWriterProvider.java
@@ -1,6 +1,7 @@
 package com.linkedin.camus.etl.kafka.common;
 
-import com.linkedin.camus.coders.CamusWrapperLight;
+import com.linkedin.camus.coders.CamusWrapperBase;
+import com.linkedin.camus.coders.KeyedCamusWrapper;
 import com.linkedin.camus.etl.IEtlKey;
 import com.linkedin.camus.etl.RecordWriterProvider;
 import com.linkedin.camus.etl.kafka.mapred.EtlMultiOutputFormat;
@@ -54,10 +55,10 @@ public class SequenceFileRecordWriterProvider implements RecordWriterProvider {
     }
 
     @Override
-    public RecordWriter<IEtlKey, CamusWrapperLight> getDataRecordWriter(
+    public RecordWriter<IEtlKey, KeyedCamusWrapper> getDataRecordWriter(
             TaskAttemptContext  context,
             String              fileName,
-            CamusWrapperLight camusWrapperLight,
+            CamusWrapperBase camusWrapperBase,
             FileOutputCommitter committer) throws IOException, InterruptedException {
 
         Configuration conf = context.getConfiguration();
@@ -104,9 +105,9 @@ public class SequenceFileRecordWriterProvider implements RecordWriterProvider {
 
         // Return a new anonymous RecordWriter that uses the
         // SequenceFile.Writer to write data to HDFS
-        return new RecordWriter<IEtlKey, CamusWrapperLight>() {
+        return new RecordWriter<IEtlKey, KeyedCamusWrapper>() {
             @Override
-            public void write(IEtlKey key, CamusWrapperLight data) throws IOException, InterruptedException {
+            public void write(IEtlKey key, KeyedCamusWrapper data) throws IOException, InterruptedException {
                 String record = (String)data.getRecord() + recordDelimiter;
                 // Use the timestamp from the EtlKey as the key for this record.
                 // TODO: Is there a better key to use here?
diff --git a/camus-etl-kafka/src/main/java/com/linkedin/camus/etl/kafka/common/StringRecordWriterProvider.java b/camus-etl-kafka/src/main/java/com/linkedin/camus/etl/kafka/common/StringRecordWriterProvider.java
index d81485f..508dbd6 100644
--- a/camus-etl-kafka/src/main/java/com/linkedin/camus/etl/kafka/common/StringRecordWriterProvider.java
+++ b/camus-etl-kafka/src/main/java/com/linkedin/camus/etl/kafka/common/StringRecordWriterProvider.java
@@ -1,6 +1,7 @@
 package com.linkedin.camus.etl.kafka.common;
 
-import com.linkedin.camus.coders.CamusWrapperLight;
+import com.linkedin.camus.coders.CamusWrapperBase;
+import com.linkedin.camus.coders.KeyedCamusWrapper;
 import com.linkedin.camus.etl.IEtlKey;
 import com.linkedin.camus.etl.RecordWriterProvider;
 import com.linkedin.camus.etl.kafka.mapred.EtlMultiOutputFormat;
@@ -69,10 +70,10 @@ public class StringRecordWriterProvider implements RecordWriterProvider {
     }
 
     @Override
-    public RecordWriter<IEtlKey, CamusWrapperLight> getDataRecordWriter(
+    public RecordWriter<IEtlKey, KeyedCamusWrapper> getDataRecordWriter(
             TaskAttemptContext  context,
             String              fileName,
-            CamusWrapperLight camusWrapperLight,
+            CamusWrapperBase camusWrapperBase,
             FileOutputCommitter committer) throws IOException, InterruptedException {
 
         // If recordDelimiter hasn't been initialized, do so now
@@ -94,6 +95,7 @@ public class StringRecordWriterProvider implements RecordWriterProvider {
         FileSystem fs = path.getFileSystem(context.getConfiguration());
         if (!isCompressed) {
             FSDataOutputStream fileOut = fs.create(path, false);
+            System.out.println("fileOut = " + fileOut);
             return new ByteRecordWriter(fileOut, recordDelimiter);
         } else {
             FSDataOutputStream fileOut = fs.create(path, false);
@@ -121,7 +123,7 @@ public class StringRecordWriterProvider implements RecordWriterProvider {
         */
     }
     
-    protected static class ByteRecordWriter extends RecordWriter<IEtlKey, CamusWrapperLight> {
+    protected static class ByteRecordWriter extends RecordWriter<IEtlKey, KeyedCamusWrapper> {
         private DataOutputStream out;
         private String recordDelimiter;
 
@@ -131,7 +133,7 @@ public class StringRecordWriterProvider implements RecordWriterProvider {
         }
 
         @Override
-        public void write(IEtlKey ignore, CamusWrapperLight value) throws IOException {
+        public void write(IEtlKey ignore, KeyedCamusWrapper value) throws IOException {
             boolean nullValue = value == null;
             if (!nullValue) {
             	String record = (String)value.getRecord() + recordDelimiter;
diff --git a/camus-etl-kafka/src/main/java/com/linkedin/camus/etl/kafka/mapred/EtlInputFormat.java b/camus-etl-kafka/src/main/java/com/linkedin/camus/etl/kafka/mapred/EtlInputFormat.java
index e078b43..609e758 100644
--- a/camus-etl-kafka/src/main/java/com/linkedin/camus/etl/kafka/mapred/EtlInputFormat.java
+++ b/camus-etl-kafka/src/main/java/com/linkedin/camus/etl/kafka/mapred/EtlInputFormat.java
@@ -1,6 +1,6 @@
 package com.linkedin.camus.etl.kafka.mapred;
 
-import com.linkedin.camus.coders.CamusWrapperLight;
+import com.linkedin.camus.coders.CamusWrapperBase;
 import com.linkedin.camus.coders.MessageDecoder;
 import com.linkedin.camus.etl.kafka.CamusJob;
 import com.linkedin.camus.etl.kafka.coders.KafkaAvroMessageDecoder;
@@ -49,7 +49,7 @@ import org.apache.log4j.Logger;
 /**
  * Input format for a Kafka pull job.
  */
-public class EtlInputFormat extends InputFormat<EtlKey, CamusWrapperLight> {
+public class EtlInputFormat extends InputFormat<EtlKey, CamusWrapperBase> {
 
 	public static final String KAFKA_BLACKLIST_TOPIC = "kafka.blacklist.topics";
 	public static final String KAFKA_WHITELIST_TOPIC = "kafka.whitelist.topics";
@@ -80,7 +80,7 @@ public class EtlInputFormat extends InputFormat<EtlKey, CamusWrapperLight> {
 	}
 
 	@Override
-	public RecordReader<EtlKey, CamusWrapperLight> createRecordReader(
+	public RecordReader<EtlKey, CamusWrapperBase> createRecordReader(
 			InputSplit split, TaskAttemptContext context) throws IOException,
 			InterruptedException {
 		return new EtlRecordReader(split, context);
diff --git a/camus-etl-kafka/src/main/java/com/linkedin/camus/etl/kafka/mapred/EtlMultiOutputCommitter.java b/camus-etl-kafka/src/main/java/com/linkedin/camus/etl/kafka/mapred/EtlMultiOutputCommitter.java
index 737a816..38e52dc 100644
--- a/camus-etl-kafka/src/main/java/com/linkedin/camus/etl/kafka/mapred/EtlMultiOutputCommitter.java
+++ b/camus-etl-kafka/src/main/java/com/linkedin/camus/etl/kafka/mapred/EtlMultiOutputCommitter.java
@@ -3,13 +3,17 @@ package com.linkedin.camus.etl.kafka.mapred;
 import java.io.BufferedOutputStream;
 import java.io.IOException;
 import java.io.OutputStream;
+import java.io.Serializable;
 import java.lang.reflect.Constructor;
 import java.util.ArrayList;
 import java.util.HashMap;
 import java.util.Map;
+import java.util.Random;
 import java.util.regex.Matcher;
 import java.util.regex.Pattern;
 
+import com.google.common.base.Joiner;
+import com.google.common.collect.Lists;
 import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
@@ -35,11 +39,12 @@ public class EtlMultiOutputCommitter extends FileOutputCommitter {
     private final RecordWriterProvider recordWriterProvider;
     private Logger log;
 
-    public void addCounts(EtlKey key) throws IOException {
-        String workingFileName = EtlMultiOutputFormat.getWorkingFileName(context, key);
-        if (!counts.containsKey(workingFileName))
+    public void addCounts(EtlKey key, String filename) throws IOException {
+        String workingFileName = EtlMultiOutputFormat.getWorkingFileName(context, key, filename);
+        if (!counts.containsKey(workingFileName)) {
             counts.put(workingFileName, new EtlCounts(key.getTopic(),
-            EtlMultiOutputFormat.getMonitorTimeGranularityMs(context)));
+                    EtlMultiOutputFormat.getMonitorTimeGranularityMs(context)));
+        }
         counts.get(workingFileName).incrementMonitorCount(key);
         addOffset(key);
     }
@@ -61,7 +66,7 @@ public class EtlMultiOutputCommitter extends FileOutputCommitter {
         } catch (Exception e) {
             throw new IllegalStateException(e);
         }
-        workingFileMetadataPattern = Pattern.compile("data\\.([^\\.]+)\\.([\\d_]+)\\.(\\d+)\\.([^\\.]+)-m-\\d+" + recordWriterProvider.getFilenameExtension());
+        workingFileMetadataPattern = Pattern.compile("data\\.([^\\.]+)\\.([\\d_]+)\\.(\\d+)\\.([^\\.]+)\\.(.*)-m-\\d+" + recordWriterProvider.getFilenameExtension());
         this.log = log;
     }
 
@@ -70,6 +75,7 @@ public class EtlMultiOutputCommitter extends FileOutputCommitter {
 
     	ArrayList<Map<String,Object>> allCountObject = new ArrayList<Map<String,Object>>();
         FileSystem fs = FileSystem.get(context.getConfiguration());
+
         if (EtlMultiOutputFormat.isRunMoveData(context)) {
             Path workPath = super.getWorkPath();
             Path baseOutDir = EtlMultiOutputFormat.getDestinationPath(context);
@@ -132,16 +138,22 @@ public class EtlMultiOutputCommitter extends FileOutputCommitter {
         String leaderId = m.group(2);
         String partition = m.group(3);
         String encodedPartition = m.group(4);
+        String filename = m.group(5);
+
+        ArrayList parts = Lists.newArrayList(
+                filename
+        );
 
         String partitionedPath =
             EtlMultiOutputFormat.getPartitioner(context, topic).generatePartitionedPath(context, topic, leaderId,
                             Integer.parseInt(partition), encodedPartition);
 
-        return partitionedPath +
-                    "/" + topic + "." + leaderId + "." + partition +
-                    "." + count+
-                    "." + offset + 
-                    "." + encodedPartition + 
-                    recordWriterProvider.getFilenameExtension();
+//        return partitionedPath +
+//                    "/" + topic + "." + leaderId + "." + partition +
+//                    "." + count+
+//                    "." + offset +
+//                    "." + encodedPartition +
+//                    recordWriterProvider.getFilenameExtension();
+        return partitionedPath + "/" + Joiner.on('.').join(parts);
     }
 }
\ No newline at end of file
diff --git a/camus-etl-kafka/src/main/java/com/linkedin/camus/etl/kafka/mapred/EtlMultiOutputFormat.java b/camus-etl-kafka/src/main/java/com/linkedin/camus/etl/kafka/mapred/EtlMultiOutputFormat.java
index c58dfaa..c5b288b 100644
--- a/camus-etl-kafka/src/main/java/com/linkedin/camus/etl/kafka/mapred/EtlMultiOutputFormat.java
+++ b/camus-etl-kafka/src/main/java/com/linkedin/camus/etl/kafka/mapred/EtlMultiOutputFormat.java
@@ -1,11 +1,14 @@
 package com.linkedin.camus.etl.kafka.mapred;
 
 import java.io.IOException;
-import java.util.ArrayList;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 
+import com.google.common.base.Joiner;
+import com.google.common.base.Preconditions;
+import com.google.common.base.Strings;
+import com.google.common.collect.Lists;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.mapreduce.JobContext;
@@ -183,11 +186,25 @@ public class EtlMultiOutputFormat extends FileOutputFormat<EtlKey, Object> {
         job.getConfiguration().setBoolean(ETL_RUN_TRACKING_POST, value);
     }
 
-    public static String getWorkingFileName(JobContext context, EtlKey key) throws IOException {
+    public static String getWorkingFileName(JobContext context, EtlKey key, String filename) throws IOException {
+        Preconditions.checkArgument(!Strings.isNullOrEmpty(filename));
         Partitioner partitioner = getPartitioner(context, key.getTopic());
-//        String topicProcessed = key.getTopic().replaceAll("\\.", "_");
-        String topicProcessed = key.getTopic();
-        return "data." + topicProcessed + "." + key.getLeaderId() + "." + key.getPartition() + "." + partitioner.encodePartition(context, key);
+        List parts = Lists.newArrayList(
+                "data",
+                key.getTopic(),
+                key.getLeaderId(),
+                key.getPartition(),
+                partitioner.encodePartition(context, key)
+        );
+        if (!Strings.isNullOrEmpty(filename)) {
+            parts.add(filename);
+        }
+
+//        List parts = Lists.newArrayList(
+//                "data",
+//                filename
+//        );
+        return Joiner.on('.').join(parts);
     }
     
     public static void setDefaultPartitioner(JobContext job, Class<?> cls) {
diff --git a/camus-etl-kafka/src/main/java/com/linkedin/camus/etl/kafka/mapred/EtlMultiOutputRecordWriter.java b/camus-etl-kafka/src/main/java/com/linkedin/camus/etl/kafka/mapred/EtlMultiOutputRecordWriter.java
index 9b34556..e76d5a1 100644
--- a/camus-etl-kafka/src/main/java/com/linkedin/camus/etl/kafka/mapred/EtlMultiOutputRecordWriter.java
+++ b/camus-etl-kafka/src/main/java/com/linkedin/camus/etl/kafka/mapred/EtlMultiOutputRecordWriter.java
@@ -1,10 +1,10 @@
 package com.linkedin.camus.etl.kafka.mapred;
 
-import java.io.IOException;
-import java.lang.reflect.Constructor;
-import java.util.HashMap;
-
-import com.linkedin.camus.coders.CamusWrapperLight;
+import com.linkedin.camus.coders.KeyedCamusWrapper;
+import com.linkedin.camus.etl.IEtlKey;
+import com.linkedin.camus.etl.RecordWriterProvider;
+import com.linkedin.camus.etl.kafka.common.EtlKey;
+import com.linkedin.camus.etl.kafka.common.ExceptionWritable;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.io.SequenceFile;
@@ -13,128 +13,137 @@ import org.apache.hadoop.mapreduce.RecordWriter;
 import org.apache.hadoop.mapreduce.TaskAttemptContext;
 import org.joda.time.DateTime;
 
-import com.linkedin.camus.etl.IEtlKey;
-import com.linkedin.camus.etl.RecordWriterProvider;
-import com.linkedin.camus.etl.kafka.common.EtlKey;
-import com.linkedin.camus.etl.kafka.common.ExceptionWritable;
+import java.io.IOException;
+import java.lang.reflect.Constructor;
+import java.util.HashMap;
 
-public class EtlMultiOutputRecordWriter extends RecordWriter<EtlKey, Object>
-{
-  private TaskAttemptContext context;
-  private Writer errorWriter = null;
-  private String currentTopic = "";
-  private long beginTimeStamp = 0;
+public class EtlMultiOutputRecordWriter extends RecordWriter<EtlKey, Object> {
+    private TaskAttemptContext context;
+    private Writer errorWriter = null;
+    private String currentTopic = "";
+    private long beginTimeStamp = 0;
 
-  private HashMap<String, RecordWriter<IEtlKey, CamusWrapperLight>> dataWriters =
-      new HashMap<String, RecordWriter<IEtlKey, CamusWrapperLight>>();
+    private HashMap<String, RecordWriter<IEtlKey, KeyedCamusWrapper>> dataWriters =
+            new HashMap<String, RecordWriter<IEtlKey, KeyedCamusWrapper>>();
 
-  private EtlMultiOutputCommitter committer;
+    private EtlMultiOutputCommitter committer;
 
-  public EtlMultiOutputRecordWriter(TaskAttemptContext context, EtlMultiOutputCommitter committer) throws IOException,
-      InterruptedException
-  {
-    this.context = context;
-    this.committer = committer;
-    errorWriter =
-        SequenceFile.createWriter(FileSystem.get(context.getConfiguration()),
-                                  context.getConfiguration(),
-                                  new Path(committer.getWorkPath(),
-                                           EtlMultiOutputFormat.getUniqueFile(context,
-                                                                              EtlMultiOutputFormat.ERRORS_PREFIX,
-                                                                              "")),
-                                  EtlKey.class,
-                                  ExceptionWritable.class);
+    public EtlMultiOutputRecordWriter(TaskAttemptContext context, EtlMultiOutputCommitter committer) throws IOException,
+            InterruptedException {
+        this.context = context;
+        this.committer = committer;
+        errorWriter =
+                SequenceFile.createWriter(FileSystem.get(context.getConfiguration()),
+                        context.getConfiguration(),
+                        new Path(committer.getWorkPath(),
+                                EtlMultiOutputFormat.getUniqueFile(context,
+                                        EtlMultiOutputFormat.ERRORS_PREFIX,
+                                        "")),
+                        EtlKey.class,
+                        ExceptionWritable.class);
 
-    if (EtlInputFormat.getKafkaMaxHistoricalDays(context) != -1)
-    {
-      int maxDays = EtlInputFormat.getKafkaMaxHistoricalDays(context);
-      beginTimeStamp = (new DateTime()).minusDays(maxDays).getMillis();
-    }
-    else
-    {
-      beginTimeStamp = 0;
+        if (EtlInputFormat.getKafkaMaxHistoricalDays(context) != -1) {
+            int maxDays = EtlInputFormat.getKafkaMaxHistoricalDays(context);
+            beginTimeStamp = (new DateTime()).minusDays(maxDays).getMillis();
+        } else {
+            beginTimeStamp = 0;
+        }
     }
-  }
 
-  @Override
-  public void close(TaskAttemptContext context) throws IOException,
-      InterruptedException
-  {
-    for (String w : dataWriters.keySet())
-    {
-      dataWriters.get(w).close(context);
+    @Override
+    public void close(TaskAttemptContext context) throws IOException,
+            InterruptedException {
+        for (String w : dataWriters.keySet()) {
+            dataWriters.get(w).close(context);
+        }
+        errorWriter.close();
     }
-    errorWriter.close();
-  }
 
-  @Override
-  public void write(EtlKey key, Object val) throws IOException,
-      InterruptedException
-  {
-    if (val instanceof CamusWrapperLight<?>)
-    {
-      if (key.getTime() < beginTimeStamp)
-      {
-        // ((Mapper.Context)context).getCounter("total",
-        // "skip-old").increment(1);
-        committer.addOffset(key);
-      }
-      else
-      {
-        if (!key.getTopic().equals(currentTopic))
-        {
-          for (RecordWriter<IEtlKey, CamusWrapperLight> writer : dataWriters.values())
-          {
-            writer.close(context);
-          }
-          dataWriters.clear();
-          currentTopic = key.getTopic();
-        }
+    @Override
+    public void write(EtlKey key, Object val) throws IOException,
+            InterruptedException {
+        if (val instanceof KeyedCamusWrapper) {
+            if (key.getTime() < beginTimeStamp) {
+                // ((Mapper.Context)context).getCounter("total",
+                // "skip-old").increment(1);
+                committer.addOffset(key);
+            } else {
+                if (!key.getTopic().equals(currentTopic)) {
+                    for (RecordWriter<IEtlKey, KeyedCamusWrapper> writer : dataWriters.values()) {
+                        writer.close(context);
+                    }
+                    dataWriters.clear();
+                    currentTopic = key.getTopic();
+                }
+
+                KeyedCamusWrapper value = (KeyedCamusWrapper) val;
+                String filename = value.getKey().toString().replaceAll("[^a-zA-Z0-9\\._]+", "_");
+                committer.addCounts(key, filename);
+                String workingFileName = EtlMultiOutputFormat.getWorkingFileName(context, key, filename);
+
+                if (!dataWriters.containsKey(workingFileName)) {
+                    RecordWriter<IEtlKey, KeyedCamusWrapper> dataRecordWriter = getDataRecordWriter(context, workingFileName, value);
 
-        committer.addCounts(key);
-        CamusWrapperLight value = (CamusWrapperLight) val;
-        String workingFileName = EtlMultiOutputFormat.getWorkingFileName(context, key);
-        if (!dataWriters.containsKey(workingFileName))
-        {
-          dataWriters.put(workingFileName, getDataRecordWriter(context, workingFileName, value));
+                    dataWriters.put(workingFileName, dataRecordWriter);
+                }
+                dataWriters.get(workingFileName).write(key, value);
+            }
+        }
+//    else if (val instanceof CamusWrapperBase<?>)
+//    {
+//      if (key.getTime() < beginTimeStamp)
+//      {
+//        // ((Mapper.Context)context).getCounter("total",
+//        // "skip-old").increment(1);
+//        committer.addOffset(key);
+//      }
+//      else
+//      {
+//        if (!key.getTopic().equals(currentTopic))
+//        {
+//          for (RecordWriter<IEtlKey, CamusWrapperBase> writer : dataWriters.values())
+//          {
+//            writer.close(context);
+//          }
+//          dataWriters.clear();
+//          currentTopic = key.getTopic();
+//        }
+//
+//        committer.addCounts(key);
+//        CamusWrapperBase value = (CamusWrapperBase) val;
+//        String workingFileName = EtlMultiOutputFormat.getWorkingFileName(context, key);
+//        if (!dataWriters.containsKey(workingFileName))
+//        {
+//          dataWriters.put(workingFileName, getDataRecordWriter(context, workingFileName, value));
+//        }
+//        dataWriters.get(workingFileName).write(key, value);
+//      }
+//    }
+        else if (val instanceof ExceptionWritable) {
+            committer.addOffset(key);
+            System.err.println(key.toString());
+            System.err.println(val.toString());
+            errorWriter.append(key, (ExceptionWritable) val);
         }
-        dataWriters.get(workingFileName).write(key, value);
-      }
-    }
-    else if (val instanceof ExceptionWritable)
-    {
-      committer.addOffset(key);
-      System.err.println(key.toString());
-      System.err.println(val.toString());
-      errorWriter.append(key, (ExceptionWritable) val);
     }
-  }
 
-  private RecordWriter<IEtlKey, CamusWrapperLight> getDataRecordWriter(TaskAttemptContext context,
-                                                                  String fileName,
-                                                                  CamusWrapperLight value) throws IOException,
-      InterruptedException
-  {
-    RecordWriterProvider recordWriterProvider = null;
-    try
-    {
-      //recordWriterProvider = EtlMultiOutputFormat.getRecordWriterProviderClass(context).newInstance();
-      Class<RecordWriterProvider> rwp = EtlMultiOutputFormat.getRecordWriterProviderClass(context);
-      Constructor<RecordWriterProvider> crwp = rwp.getConstructor(TaskAttemptContext.class);
-      recordWriterProvider = crwp.newInstance(context);
-    }
-    catch (InstantiationException e)
-    {
-      throw new IllegalStateException(e);
-    }
-    catch (IllegalAccessException e)
-    {
-      throw new IllegalStateException(e);
-    }
-    catch (Exception e) 
-    {
-        throw new IllegalStateException(e);
+    private RecordWriter<IEtlKey, KeyedCamusWrapper> getDataRecordWriter(TaskAttemptContext context,
+                                                                         String fileName,
+                                                                         KeyedCamusWrapper value) throws IOException,
+            InterruptedException {
+        RecordWriterProvider recordWriterProvider = null;
+        try {
+            //recordWriterProvider = EtlMultiOutputFormat.getRecordWriterProviderClass(context).newInstance();
+            Class<RecordWriterProvider> rwp = EtlMultiOutputFormat.getRecordWriterProviderClass(context);
+            Constructor<RecordWriterProvider> crwp = rwp.getConstructor(TaskAttemptContext.class);
+            recordWriterProvider = crwp.newInstance(context);
+        } catch (InstantiationException e) {
+            throw new IllegalStateException(e);
+        } catch (IllegalAccessException e) {
+            throw new IllegalStateException(e);
+        } catch (Exception e) {
+            throw new IllegalStateException(e);
+        }
+        return recordWriterProvider.getDataRecordWriter(context, fileName, value, committer);
     }
-    return recordWriterProvider.getDataRecordWriter(context, fileName, value, committer);
-  }
 }
diff --git a/camus-etl-kafka/src/main/java/com/linkedin/camus/etl/kafka/mapred/EtlRecordReader.java b/camus-etl-kafka/src/main/java/com/linkedin/camus/etl/kafka/mapred/EtlRecordReader.java
index 9cd883b..5b0291e 100644
--- a/camus-etl-kafka/src/main/java/com/linkedin/camus/etl/kafka/mapred/EtlRecordReader.java
+++ b/camus-etl-kafka/src/main/java/com/linkedin/camus/etl/kafka/mapred/EtlRecordReader.java
@@ -1,8 +1,6 @@
 package com.linkedin.camus.etl.kafka.mapred;
 
-import com.linkedin.camus.coders.CamusWrapper;
-import com.linkedin.camus.coders.CamusWrapperLight;
-import com.linkedin.camus.coders.MessageDecoder;
+import com.linkedin.camus.coders.*;
 import com.linkedin.camus.etl.kafka.CamusJob;
 import com.linkedin.camus.etl.kafka.coders.MessageDecoderFactory;
 import com.linkedin.camus.etl.kafka.common.EtlKey;
@@ -27,7 +25,7 @@ import org.apache.hadoop.mapreduce.TaskAttemptContext;
 import org.apache.log4j.Logger;
 import org.joda.time.DateTime;
 
-public class EtlRecordReader extends RecordReader<EtlKey, CamusWrapperLight> {
+public class EtlRecordReader extends RecordReader<EtlKey, CamusWrapperBase> {
     private static final String PRINT_MAX_DECODER_EXCEPTIONS = "max.decoder.exceptions.to.print";
     private static final String DEFAULT_SERVER = "server";
     private static final String DEFAULT_SERVICE = "service";
@@ -40,11 +38,12 @@ public class EtlRecordReader extends RecordReader<EtlKey, CamusWrapperLight> {
     private long readBytes = 0;
 
     private boolean skipSchemaErrors = false;
-    private MessageDecoder decoder;
+//    private MessageDecoder decoder;
+    private KeyedMessageDecoder decoder;
     private final BytesWritable msgValue = new BytesWritable();
     private final BytesWritable msgKey = new BytesWritable();
     private final EtlKey key = new EtlKey();
-    private CamusWrapperLight value;
+    private CamusWrapperBase value;
 
     private int maxPullHours = 0;
     private int exceptionCount = 0;
@@ -120,10 +119,10 @@ public class EtlRecordReader extends RecordReader<EtlKey, CamusWrapperLight> {
         }
     }
 
-    private CamusWrapper getWrappedRecord(String topicName, byte[] payload) throws IOException {
-        CamusWrapper r = null;
+    private KeyedCamusWrapper getWrappedRecord(String topicName, byte[] key, byte[] payload) throws IOException {
+        KeyedCamusWrapper r = null;
         try {
-            r = decoder.decode(payload);
+            r = decoder.decode(key, payload);
         } catch (Exception e) {
             if (!skipSchemaErrors) {
                 throw new IOException(e);
@@ -175,7 +174,7 @@ public class EtlRecordReader extends RecordReader<EtlKey, CamusWrapperLight> {
     }
 
     @Override
-    public CamusWrapperLight getCurrentValue() throws IOException, InterruptedException {
+    public CamusWrapperBase getCurrentValue() throws IOException, InterruptedException {
         return value;
     }
 
@@ -226,7 +225,7 @@ public class EtlRecordReader extends RecordReader<EtlKey, CamusWrapperLight> {
                             CamusJob.getKafkaTimeoutValue(mapperContext),
                             CamusJob.getKafkaBufferSize(mapperContext));
 
-                    decoder = MessageDecoderFactory.createMessageDecoder(context, request.getTopic());
+                    decoder = (KeyedMessageDecoder) MessageDecoderFactory.createMessageDecoder(context, request.getTopic());
                 }
                 int count = 0;
                 while (reader.getNext(key, msgValue, msgKey)) {
@@ -250,9 +249,9 @@ public class EtlRecordReader extends RecordReader<EtlKey, CamusWrapperLight> {
                     }
 
                     long tempTime = System.currentTimeMillis();
-                    CamusWrapperLight wrapper;
+                    CamusWrapperBase wrapper;
                     try {
-                        wrapper = getWrappedRecord(key.getTopic(), bytes);
+                        wrapper = getWrappedRecord(key.getTopic(), keyBytes, bytes);
                     } catch (Exception e) {
                         if (exceptionCount < getMaximumDecoderExceptionsToPrint(context)) {
                             mapperContext.write(key, new ExceptionWritable(e));
diff --git a/camus-kafka-coders/src/main/java/com/linkedin/camus/etl/kafka/coders/KafkaAvroMessageDecoder.java b/camus-kafka-coders/src/main/java/com/linkedin/camus/etl/kafka/coders/KafkaAvroMessageDecoder.java
index a67cf12..630134b 100644
--- a/camus-kafka-coders/src/main/java/com/linkedin/camus/etl/kafka/coders/KafkaAvroMessageDecoder.java
+++ b/camus-kafka-coders/src/main/java/com/linkedin/camus/etl/kafka/coders/KafkaAvroMessageDecoder.java
@@ -1,25 +1,25 @@
 package com.linkedin.camus.etl.kafka.coders;
 
-import java.io.IOException;
-import java.nio.ByteBuffer;
-import java.util.Properties;
-
-import kafka.message.Message;
-
+import com.google.common.base.Charsets;
+import com.linkedin.camus.coders.CamusWrapper;
+import com.linkedin.camus.coders.KeyedCamusWrapper;
+import com.linkedin.camus.coders.KeyedMessageDecoder;
+import com.linkedin.camus.coders.MessageDecoderException;
+import com.linkedin.camus.schemaregistry.CachedSchemaRegistry;
+import com.linkedin.camus.schemaregistry.SchemaRegistry;
 import org.apache.avro.Schema;
 import org.apache.avro.generic.GenericData.Record;
 import org.apache.avro.generic.GenericDatumReader;
 import org.apache.avro.io.DatumReader;
 import org.apache.avro.io.DecoderFactory;
-
-import com.linkedin.camus.coders.CamusWrapper;
-import com.linkedin.camus.coders.MessageDecoder;
-import com.linkedin.camus.coders.MessageDecoderException;
-import com.linkedin.camus.schemaregistry.CachedSchemaRegistry;
-import com.linkedin.camus.schemaregistry.SchemaRegistry;
 import org.apache.hadoop.io.Text;
 
-public class KafkaAvroMessageDecoder extends MessageDecoder<byte[], Record> {
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.Date;
+import java.util.Properties;
+
+public class KafkaAvroMessageDecoder extends KeyedMessageDecoder<byte[], String, byte[], Record> {
 	protected DecoderFactory decoderFactory;
 	protected SchemaRegistry<Schema> registry;
 	private Schema latestSchema;
@@ -43,7 +43,30 @@ public class KafkaAvroMessageDecoder extends MessageDecoder<byte[], Record> {
         decoderFactory = DecoderFactory.get();
 	}
 
-	private class MessageDecoderHelper {
+    @Override
+    public CamusWrapper<byte[]> decode(Record message) {
+        throw new RuntimeException("Not implemented");
+    }
+
+    @Override
+    public KeyedCamusWrapper<String, Record> decode(byte[] key, byte[] message) {
+        try {
+            MessageDecoderHelper helper = new MessageDecoderHelper(registry,
+                    topicName, message).invoke();
+            DatumReader<Record> reader = (helper.getTargetSchema() == null) ? new GenericDatumReader<Record>(
+                    helper.getSchema()) : new GenericDatumReader<Record>(
+                    helper.getSchema(), helper.getTargetSchema());
+
+            Record read = reader.read(null, decoderFactory.binaryDecoder(helper.getBuffer().array(),
+                    helper.getStart(), helper.getLength(), null));
+            return new KeyedCamusWrapper(new String(key, Charsets.UTF_8), read, new Date().getTime());
+
+        } catch (IOException e) {
+            throw new MessageDecoderException(e);
+        }
+    }
+
+    protected class MessageDecoderHelper {
 		//private Message message;
 		private ByteBuffer buffer;
 		private Schema schema;
@@ -85,8 +108,8 @@ public class KafkaAvroMessageDecoder extends MessageDecoder<byte[], Record> {
 
 		private ByteBuffer getByteBuffer(byte[] payload) {
 			ByteBuffer buffer = ByteBuffer.wrap(payload);
-			if (buffer.get() != MAGIC_BYTE)
-				throw new IllegalArgumentException("Unknown magic byte!");
+//			if (buffer.get() != MAGIC_BYTE)
+//				throw new IllegalArgumentException("Unknown magic byte!");
 			return buffer;
 		}
 
@@ -98,7 +121,7 @@ public class KafkaAvroMessageDecoder extends MessageDecoder<byte[], Record> {
 				throw new IllegalStateException("Unknown schema id: " + id);
 
 			start = buffer.position() + buffer.arrayOffset();
-			length = buffer.limit() - 5;
+			length = buffer.limit() - 4;
 
 			// try to get a target schema, if any
 			targetSchema = latestSchema;
@@ -106,23 +129,6 @@ public class KafkaAvroMessageDecoder extends MessageDecoder<byte[], Record> {
 		}
 	}
 
-	public CamusWrapper<Record> decode(byte[] payload) {
-		try {
-			MessageDecoderHelper helper = new MessageDecoderHelper(registry,
-					topicName, payload).invoke();
-			DatumReader<Record> reader = (helper.getTargetSchema() == null) ? new GenericDatumReader<Record>(
-					helper.getSchema()) : new GenericDatumReader<Record>(
-					helper.getSchema(), helper.getTargetSchema());
-
-			return new CamusAvroWrapper(reader.read(null, decoderFactory
-                    .binaryDecoder(helper.getBuffer().array(),
-                            helper.getStart(), helper.getLength(), null)));
-	
-		} catch (IOException e) {
-			throw new MessageDecoderException(e);
-		}
-	}
-
 	public static class CamusAvroWrapper extends CamusWrapper<Record> {
 
 	    public CamusAvroWrapper(Record record) {
diff --git a/camus-kafka-coders/src/main/java/com/linkedin/camus/etl/kafka/coders/LatestSchemaKafkaAvroMessageDecoder.java b/camus-kafka-coders/src/main/java/com/linkedin/camus/etl/kafka/coders/LatestSchemaKafkaAvroMessageDecoder.java
index 3247d45..f9d7fdc 100644
--- a/camus-kafka-coders/src/main/java/com/linkedin/camus/etl/kafka/coders/LatestSchemaKafkaAvroMessageDecoder.java
+++ b/camus-kafka-coders/src/main/java/com/linkedin/camus/etl/kafka/coders/LatestSchemaKafkaAvroMessageDecoder.java
@@ -1,44 +1,39 @@
 package com.linkedin.camus.etl.kafka.coders;
 
+import com.google.common.base.Charsets;
+import com.linkedin.camus.coders.KeyedCamusWrapper;
 import kafka.message.Message;
-
 import org.apache.avro.Schema;
 import org.apache.avro.generic.GenericData.Record;
 import org.apache.avro.generic.GenericDatumReader;
-import org.apache.hadoop.conf.Configuration;
 
-import com.linkedin.camus.coders.CamusWrapper;
+public class LatestSchemaKafkaAvroMessageDecoder extends KafkaAvroMessageDecoder {
+
+    @Override
+    public KeyedCamusWrapper<String, Record> decode(byte[] key, byte[] message) {
+        try {
+            GenericDatumReader<Record> reader = new GenericDatumReader<Record>();
+
+            Schema schema = super.registry.getLatestSchemaByTopic(super.topicName).getSchema();
 
-public class LatestSchemaKafkaAvroMessageDecoder extends KafkaAvroMessageDecoder
-{
+            reader.setSchema(schema);
 
-	@Override
-	public CamusWrapper<Record> decode(byte[] payload)
-	{
-		try
-		{
-			GenericDatumReader<Record> reader = new GenericDatumReader<Record>();
-			
-			Schema schema = super.registry.getLatestSchemaByTopic(super.topicName).getSchema();
-			
-			reader.setSchema(schema);
-			
-			return new CamusWrapper<Record>(reader.read(
-                    null, 
+            Record record = reader.read(
+                    null,
                     decoderFactory.jsonDecoder(
-                            schema, 
+                            schema,
                             new String(
-                                    payload, 
+                                    message,
                                     //Message.payloadOffset(message.magic()),
                                     Message.MagicOffset(),
-                                    payload.length - Message.MagicOffset()
+                                    message.length - Message.MagicOffset()
                             )
                     )
-            ));
-		}
-		catch (Exception e)
-		{
-			throw new RuntimeException(e);
-		}
-	}
+            );
+            String keyStr = new String(key, Charsets.UTF_8);
+            return new KeyedCamusWrapper<>(keyStr, record, -1);
+        } catch (Exception e) {
+            throw new RuntimeException(e);
+        }
+    }
 }
\ No newline at end of file
diff --git a/pom.xml b/pom.xml
index 68949d9..1b13cc1 100644
--- a/pom.xml
+++ b/pom.xml
@@ -211,7 +211,17 @@
 					</execution>
 				</executions>
 			</plugin>
-		</plugins>
+            <plugin>
+                <groupId>org.apache.maven.plugins</groupId>
+                <artifactId>maven-compiler-plugin</artifactId>
+                <version>3.1</version>
+                <configuration>
+                    <source>1.7</source>
+                    <target>1.7</target>
+                    <encoding>UTF-8</encoding>
+                </configuration>
+            </plugin>
+        </plugins>
 	</build>
 
 </project>
